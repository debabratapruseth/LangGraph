{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##LangGraph-based intelligent agent demo\n"
      ],
      "metadata": {
        "id": "XGIS9_bDUEtW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gakpCoJKCLLp",
        "outputId": "b1f1ddba-f2cc-4660-a2f4-793b2b474f02",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.5.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.72-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.93.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langgraph-0.5.2-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.72-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, pypdf, ormsgpack, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, langgraph-sdk, dataclasses-json, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-openai-0.3.27 langgraph-0.5.2 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.72 marshmallow-3.26.1 mypy-extensions-1.1.0 ormsgpack-1.10.0 pydantic-settings-2.10.1 pypdf-5.7.0 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langgraph langchain-openai pypdf requests langchain-community faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== SYSTEM SETUP AND IMPORTS ====\n",
        "\n",
        "import os  # For setting environment variables\n",
        "import requests  # For making HTTP requests (used to fetch weather data)\n",
        "\n",
        "# LangChain import for OpenAI's LLM interface (Chat model)\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# LangChain components to define and format prompts\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# LangChain component to chain together an LLM with a prompt\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# LangChain utilities to create modular pipeline steps\n",
        "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
        "\n",
        "# LangChain document processing: for reading and chunking PDF files\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# OpenAI embeddings to convert text chunks into vector representations\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# FAISS vector database: used for similarity search on text chunks\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# RetrievalQA enables question-answering over a vector database (RAG)\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Memory buffer to store multi-turn conversation history\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# LangGraph library to build stateful workflows using a graph-based controller\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# OpenAI Python SDK to use DALL·E for image generation\n",
        "from openai import OpenAI\n",
        "\n",
        "# IPython display for rendering images inside Google Colab notebooks\n",
        "from IPython.display import Image, display\n",
        "\n",
        "\n",
        "# ==== MEMORY AND API KEY SETUP ====\n",
        "\n",
        "# Initialize a memory buffer that stores chat history between user and assistant\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# Load OpenAI and Weather API keys from secure Colab environment\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"WEATHER_API_KEY\"] = userdata.get(\"WEATHER_API_KEY\")"
      ],
      "metadata": {
        "id": "QU-OOKWkl-O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up LLM with loaded API key\n",
        "# This initializes a ChatOpenAI instance using the API key, with temperature=0 for consistent responses\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "## Prompt to extract cities\n",
        "# This prompt tells the LLM to find only city names from a text input (extracted PDF)\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are a city name extraction expert. Carefully read the loaded PDF and extract **only city names**.\n",
        "Return the result as a comma-separated list. Ignore repeated cities.\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\")\n",
        "\n",
        "# Chain the prompt with the LLM. When invoked, this will extract cities from input text.\n",
        "city_extractor = prompt | llm\n",
        "\n",
        "## Prompt to call for action\n",
        "# This prompt defines possible user intents and instructs the LLM to classify input into one of them\n",
        "intent_prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are a controller for a banner generation assistant.\n",
        "\n",
        "Based on the user input below, choose the correct action.\n",
        "\n",
        "Available actions:\n",
        "- get_weather: If user wants weather for a city\n",
        "- get_agenda: If user wants to update or change the agenda\n",
        "- handle_rag: If user is asking a question about the document\n",
        "- ask_city: If they want to change the city\n",
        "- end: If the user wants to quit\n",
        "\n",
        "Respond with only the action name.\n",
        "\n",
        "User: \"{input}\"\n",
        "\"\"\")\n",
        "\n",
        "## LLMChain combines the intent prompt and LLM with memory to track conversation context\n",
        "intent_chain = LLMChain(llm=llm, prompt=intent_prompt, memory=memory)\n",
        "\n",
        "## Prompt to recommend\n",
        "# This prompt generates a one-sentence recommendation based on city, weather, and agenda\n",
        "recommendation_prompt = PromptTemplate.from_template(\"\"\"\n",
        "Given the following workshop details, suggest a suitable setting (e.g., indoor/outdoor, park/hall).\n",
        "\n",
        "City: {city}\n",
        "Weather: {weather}\n",
        "Agenda: {agenda}\n",
        "\n",
        "Respond in one sentence with a clear recommendation.\n",
        "\"\"\")\n",
        "\n",
        "# LLMChain to run the recommendation prompt with user context\n",
        "recommendation_chain = LLMChain(llm=llm, prompt=recommendation_prompt)\n",
        "\n",
        "\n",
        "\n",
        "# Function to extract text from a given PDF file\n",
        "# Returns both the combined text and individual page objects\n",
        "def extract_text_from_pdf(file_path):\n",
        "    loader = PyPDFLoader(file_path)  # Load PDF\n",
        "    pages = loader.load_and_split()  # Split into individual pages\n",
        "    text = \"\\n\".join([p.page_content for p in pages])  # Combine all page content\n",
        "    return text, pages\n",
        "\n",
        "# Function to set up a RAG pipeline\n",
        "# Uses OpenAI embeddings and FAISS to create a vectorstore retriever\n",
        "def setup_retriever(pages):\n",
        "    embeddings = OpenAIEmbeddings()  # Generate vector embeddings for pages\n",
        "    vectorstore = FAISS.from_documents(pages, embeddings)  # Store in FAISS DB\n",
        "    return RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())  # Build RAG chain\n",
        "\n",
        "# Function to get weather data using Weather API for a given city\n",
        "def get_weather(city):\n",
        "    api_key = os.environ.get(\"WEATHER_API_KEY\")\n",
        "    url = f\"http://api.weatherapi.com/v1/current.json?key={api_key}&q={city}\"\n",
        "    r = requests.get(url)  # Send HTTP request\n",
        "\n",
        "    if r.status_code == 200:\n",
        "        data = r.json()\n",
        "        temp = data['current']['temp_c']\n",
        "        desc = data['current']['condition']['text']\n",
        "        return f\"\\U0001F324\\uFE0F {city}: {temp}°C, {desc}\"  # ☀️ Weather emoji and weather report\n",
        "    else:\n",
        "        # Return an error message if the API call fails\n",
        "        return f\"❌ Could not get weather for {city} — {r.status_code}: {r.text}\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFH58MHcLhmb",
        "outputId": "c0615dbe-8196-43b0-fbb1-3b341782181f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2209210393.py:38: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  intent_chain = LLMChain(llm=llm, prompt=intent_prompt, memory=memory)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Extract cities from PDF and store in state\n",
        "# In this demo code i am asking user to upload a PDF file with namr of city , countries. You can use other approach also like calling a API call to get list of city, uploading excel etc.\n",
        "def extract_cities_node(state):\n",
        "    print(\"📄 Extracting city names from PDF...\")\n",
        "    pdf_text = state[\"pdf_text\"]\n",
        "    # Use the city_extractor (LLM + prompt) to extract city names from the text\n",
        "    raw_output = city_extractor.invoke({\"text\": pdf_text})\n",
        "    # Convert the output string into a list of cities (removing spaces and empty values)\n",
        "    cities = [city.strip() for city in raw_output.content.split(\",\") if city.strip()]\n",
        "    print(f\"Found {len(cities)} cities.\")\n",
        "    # Save the extracted cities in the workflow state\n",
        "    state[\"cities\"] = cities\n",
        "    return state\n",
        "\n",
        "# 2. Ask user to pick a city\n",
        "def ask_city_node(state):\n",
        "    print(\"\\n📍 Cities found:\")\n",
        "    # Handle case where no cities were found in the previous step\n",
        "    if not state.get(\"cities\"):\n",
        "        print(\"No cities found to choose from.\")\n",
        "        return state  # Skip or exit gracefully\n",
        "\n",
        "    # Display city options with a number\n",
        "    for i, city in enumerate(state[\"cities\"]):\n",
        "        print(f\"{i+1}. {city}\")\n",
        "\n",
        "    # Prompt user to select a city by number\n",
        "    while True:\n",
        "        try:\n",
        "            choice_input = input(\"Choose a city NUMBER: \")\n",
        "            choice = int(choice_input) - 1\n",
        "            if 0 <= choice < len(state[\"cities\"]):\n",
        "                state[\"chosen_city\"] = state[\"cities\"][choice]\n",
        "                return state\n",
        "            else:\n",
        "                print(\"Invalid city number. Please choose a number from the list.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "# 3. Get weather for selected city\n",
        "def get_weather_node(state):\n",
        "    city = state[\"chosen_city\"]\n",
        "    # If no city was selected, skip weather fetch\n",
        "    if not city:\n",
        "        print(\"No city was chosen to get weather for.\")\n",
        "        return state\n",
        "\n",
        "    # Fetch and store weather information using API\n",
        "    result = get_weather(city)\n",
        "    print(\"🌈 Weather Info:\", result)\n",
        "    state[\"weather_info\"] = result\n",
        "    return state\n",
        "\n",
        "# 4. Ask if user wants to continue\n",
        "def ask_continue_node(state):\n",
        "    # Prompt the user whether they want to process another city\n",
        "    response = input(\"🔁 Do you want to generate a banner for another city? (yes/no): \").lower().strip()\n",
        "    state[\"continue_flag\"] = response in [\"yes\", \"y\"]\n",
        "    return state\n",
        "\n",
        "# 5. DALLE\n",
        "def generate_banner_node(state):\n",
        "    print(\"🧠 Generating banner image...\")\n",
        "\n",
        "    # Create OpenAI client with API key\n",
        "    openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "    # Prepare prompt using context: city, weather, and agenda\n",
        "    city = state[\"chosen_city\"]\n",
        "    weather = state[\"weather_info\"]\n",
        "    agenda = state[\"agenda\"]\n",
        "\n",
        "    prompt = f\"Design a banner image for a workshop in {city}. Weather is {weather}. Agenda is: {agenda}.\"\n",
        "\n",
        "    try:\n",
        "        # Call OpenAI DALL·E API to generate an image\n",
        "        response = client.images.generate(\n",
        "            model=\"dall-e-3\",\n",
        "            prompt=prompt,\n",
        "            size=\"1024x1024\",\n",
        "            n=1\n",
        "        )\n",
        "        image_url = response.data[0].url\n",
        "        state[\"image_url\"] = image_url\n",
        "\n",
        "        print(\"🖼️ Image created:\", image_url)\n",
        "        # Show image inside notebook (for Colab environments)\n",
        "        display(Image(url=image_url))\n",
        "    except Exception as e:\n",
        "        state[\"image_url\"] = f\"❌ Failed to generate image: {e}\"\n",
        "        print(state[\"image_url\"])\n",
        "\n",
        "    return state\n",
        "\n",
        "# 6. Agenda\n",
        "def get_agenda_node(state):\n",
        "    # Ask the user for a workshop agenda (used for image and recommendation)\n",
        "    agenda = input(\"📝 What is the agenda of the workshop/meeting?: \").strip()\n",
        "    state[\"agenda\"] = agenda\n",
        "    return state\n",
        "\n",
        "# 7. classify final request with intent\n",
        "# This node identifies the user's next intent and routes accordingly\n",
        "def classify_intent_node(state):\n",
        "    user_input = input(\"💬 What would you like to do next? \").strip()\n",
        "    llm_response = intent_chain.invoke({\"input\": user_input})\n",
        "\n",
        "    # Handle different LLM output formats (dict or plain text)\n",
        "    intent_raw = llm_response.get(\"text\") if isinstance(llm_response, dict) else llm_response\n",
        "    intent = intent_raw.strip().lower()\n",
        "\n",
        "    # Fallback logic in case LLM returns an unrecognized intent\n",
        "    valid_intents = [\"get_agenda\", \"ask_city\", \"handle_rag\", \"end\"]\n",
        "    if intent not in valid_intents:\n",
        "        print(\"⚠️ Sorry, I didn't understand that. You can try:\")\n",
        "        print(\"- 'Change agenda'\\n- 'New city'\\n- 'Ask about the PDF'\\n- 'Exit'\")\n",
        "        intent = \"classify_intent\"  # Stay on this node\n",
        "\n",
        "    print(\"🤖 Detected intent:\", intent)\n",
        "    state[\"intent\"] = intent\n",
        "    state[\"user_input\"] = user_input\n",
        "\n",
        "    # Log conversation into memory buffer\n",
        "    memory.chat_memory.add_user_message(user_input)\n",
        "    memory.chat_memory.add_ai_message(f\"Intent detected: {intent}\")\n",
        "    return state\n",
        "\n",
        "# 8. RAG node\n",
        "def handle_rag_node(state):\n",
        "    print(\"📚 Asking your question to the PDF...\")\n",
        "    qa = state[\"qa\"]  # The QA pipeline created earlier\n",
        "    question = state[\"user_input\"]\n",
        "    answer = qa.run(question)  # Query the vector database\n",
        "    print(\"📖 Answer:\", answer)\n",
        "    return state\n",
        "\n",
        "# 9. Recommendation node\n",
        "def recommendation_node(state):\n",
        "    city = state.get(\"chosen_city\", \"\")\n",
        "    weather = state.get(\"weather_info\", \"\")\n",
        "    agenda = state.get(\"agenda\", \"\")\n",
        "\n",
        "    print(\"\\n🔎 Generating event recommendation...\")\n",
        "\n",
        "    # Generate recommendation using city + weather + agenda\n",
        "    response = recommendation_chain.run({\n",
        "        \"city\": city,\n",
        "        \"weather\": weather,\n",
        "        \"agenda\": agenda\n",
        "    }).strip()\n",
        "\n",
        "    print(\"✅ Recommendation:\", response)\n",
        "    state[\"recommendation\"] = response\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "xUN49XFmL44T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List\n",
        "\n",
        "# Define the structure of the graph state using Python typing\n",
        "# This ensures each state transition has access to shared variables\n",
        "class GraphState(TypedDict, total=False):\n",
        "    city: str                 # Current city name\n",
        "    weather_info: str         # Weather data string for the chosen city\n",
        "    agenda: str               # User-defined agenda for the workshop\n",
        "    image_url: str            # URL of the generated banner image\n",
        "    pdf_text: str             # Raw text content extracted from the PDF\n",
        "    cities: list[str]         # List of extracted city names\n",
        "    chosen_city: str          # City selected by the user\n",
        "    continue_flag: bool       # Whether user wants to generate another banner\n",
        "    qa: object                # The RetrievalQA object for document-based questions\n",
        "    user_input: str           # Last user input message\n",
        "    intent: str               # Classified intent from user's message\n",
        "    recommendation: str       # LLM-generated event recommendation\n",
        "\n",
        "# Create a LangGraph workflow using the above state structure\n",
        "graph = StateGraph(GraphState)\n",
        "\n",
        "# Add each node (function) to the workflow graph\n",
        "# These are the core processing steps of the assistant\n",
        "graph.add_node(\"extract_cities\", extract_cities_node)           # Extract city names from the uploaded PDF\n",
        "graph.add_node(\"ask_city\", ask_city_node)                       # Ask user to select a city\n",
        "graph.add_node(\"get_weather\", get_weather_node)                 # Fetch weather data for selected city\n",
        "graph.add_node(\"get_agenda\", get_agenda_node)                   # Ask user to enter the workshop agenda\n",
        "graph.add_node(\"generate_banner\", generate_banner_node)         # Generate an image using DALL·E\n",
        "graph.add_node(\"classify_intent\", classify_intent_node)         # Determine what the user wants to do next\n",
        "graph.add_node(\"handle_rag\", handle_rag_node)                   # Answer document-based questions using RAG\n",
        "graph.add_node(\"recommendation\", recommendation_node)           # Recommend a suitable setting (e.g., park or hall)\n",
        "\n",
        "\n",
        "# Define the entry point for the graph — this is where execution begins\n",
        "graph.set_entry_point(\"extract_cities\")\n",
        "\n",
        "# Define the default sequential flow between the steps\n",
        "graph.add_edge(\"extract_cities\", \"ask_city\")                     # After extracting cities, prompt user to pick one\n",
        "graph.add_edge(\"ask_city\", \"get_weather\")                        # Then get weather info for the selected city\n",
        "graph.add_edge(\"get_weather\", \"recommendation\")                  # Generate event recommendation based on weather\n",
        "graph.add_edge(\"recommendation\", \"get_agenda\")                   # Then ask user for the agenda\n",
        "graph.add_edge(\"get_agenda\", \"generate_banner\")                  # Generate a banner image using DALL·E\n",
        "graph.add_edge(\"generate_banner\", \"classify_intent\")             # Ask user what they want to do next\n",
        "\n",
        "# Add branching logic based on the user's intent (controller node)\n",
        "# The result of classify_intent_node determines the next node\n",
        "graph.add_conditional_edges(\n",
        "    \"classify_intent\",\n",
        "    lambda state: {\n",
        "        \"get_agenda\": \"get_agenda\",          # User wants to update agenda\n",
        "        \"ask_city\": \"ask_city\",              # User wants to change the city\n",
        "        \"handle_rag\": \"handle_rag\",          # User wants to ask questions about the document\n",
        "        \"end\": END                           # End the session\n",
        "    }.get(state[\"intent\"], \"classify_intent\")  # If intent unrecognized, stay in classify_intent\n",
        ")\n",
        "\n",
        "# If user asks a question (RAG), return to intent classification afterwards\n",
        "graph.add_edge(\"handle_rag\", \"classify_intent\")\n",
        "\n",
        "# Compile the graph into an executable object\n",
        "graph_executor = graph.compile()\n"
      ],
      "metadata": {
        "id": "kcWYT7wUMMUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Welcome message for the user\n",
        "print(\"Hi! I am the global meeting planner for your organization.\")\n",
        "print(\"Please upload your PDF file with the list of cities your organization operates: \")\n",
        "\n",
        "# Allow user to upload a PDF file via Colab UI\n",
        "uploaded_pdf = files.upload()\n",
        "\n",
        "# Get the filename (first file from upload)\n",
        "pdf_path = list(uploaded_pdf.keys())[0]\n",
        "\n",
        "# Extract raw text and page objects from the uploaded PDF\n",
        "pdf_text, pages = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Set up the RAG retriever using the pages (used later for document Q&A)\n",
        "retriever = setup_retriever(pages)\n",
        "\n",
        "# Initialize the graph state with default values\n",
        "initial_state = {\n",
        "    \"pdf_text\": pdf_text,     # Full text from the PDF\n",
        "    \"qa\": retriever,          # RAG object for Q&A\n",
        "    \"cities\": [],             # Placeholder for extracted city names\n",
        "    \"chosen_city\": \"\",        # Will be filled when user picks a city\n",
        "    \"continue_checking\": False  # (Unused) Flag for looping, could support multi-round usage\n",
        "}\n",
        "\n",
        "# Execute the LangGraph workflow with the initial state\n",
        "# This starts from the 'extract_cities' node and follows the graph logic\n",
        "final_state = graph_executor.invoke(initial_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "enOLHGo-MKYX",
        "outputId": "996f7566-96bd-4629-f8bd-8d38d4ab9584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! I am the global meeting planner for your organization.\n",
            "Please upload your PDF file with the list of cities your organization operates: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-43e61fc1-7755-4bda-9560-7a03f392b341\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-43e61fc1-7755-4bda-9560-7a03f392b341\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving SCb Office.pdf to SCb Office.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2209210393.py:68: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings()  # Generate vector embeddings for pages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Extracting city names from PDF...\n",
            "Found 30 cities.\n",
            "\n",
            "📍 Cities found:\n",
            "1. London\n",
            "2. New York\n",
            "3. Houston\n",
            "4. Singapore\n",
            "5. Hong Kong\n",
            "6. Dhaka\n",
            "7. Paris\n",
            "8. Frankfurt\n",
            "9. Warsaw\n",
            "10. Stockholm\n",
            "11. Nairobi\n",
            "12. Accra\n",
            "13. Gaborone\n",
            "14. Port Louis\n",
            "15. Lagos\n",
            "16. Lusaka\n",
            "17. Dar-es-Salaam\n",
            "18. Kampala\n",
            "19. Ho Chi Minh City\n",
            "20. Yangon\n",
            "21. Colombo\n",
            "22. Kathmandu\n",
            "23. Karachi\n",
            "24. Doha\n",
            "25. Riyadh\n",
            "26. Dubai\n",
            "27. Baghdad\n",
            "28. Bahrain\n",
            "29. Abidjan\n",
            "30. Cairo\n",
            "Choose a city NUMBER: 4\n",
            "🌈 Weather Info: 🌤️ Singapore: 29.4°C, Partly cloudy\n",
            "\n",
            "🔎 Generating event recommendation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-1658199746.py:146: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = recommendation_chain.run({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Recommendation: An indoor setting, such as a conference hall, would be suitable for the workshop in Singapore due to the warm weather and potential for rain.\n",
            "📝 What is the agenda of the workshop/meeting?: Artificial Intelligence\n",
            "🧠 Generating banner image...\n",
            "🖼️ Image created: https://oaidalleapiprodscus.blob.core.windows.net/private/org-39LAhQ4efzWBgsvKRvFs3Rao/user-PwfybX5L6lUZPyfB2uzqiWs8/img-fLyXOCTZ0uxRvw3wi3v1KcmP.png?st=2025-07-11T14%3A54%3A55Z&se=2025-07-11T16%3A54%3A55Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=4ab8dc02-4155-4914-980a-5346f458538c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-07-11T02%3A45%3A40Z&ske=2025-07-12T02%3A45%3A40Z&sks=b&skv=2024-08-04&sig=flQJkPygw3Z41ShqpAROSQLUyqa3AiXZzUL7CkKx8Sg%3D\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-39LAhQ4efzWBgsvKRvFs3Rao/user-PwfybX5L6lUZPyfB2uzqiWs8/img-fLyXOCTZ0uxRvw3wi3v1KcmP.png?st=2025-07-11T14%3A54%3A55Z&se=2025-07-11T16%3A54%3A55Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=4ab8dc02-4155-4914-980a-5346f458538c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-07-11T02%3A45%3A40Z&ske=2025-07-12T02%3A45%3A40Z&sks=b&skv=2024-08-04&sig=flQJkPygw3Z41ShqpAROSQLUyqa3AiXZzUL7CkKx8Sg%3D\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💬 What would you like to do next? Change agenda\n",
            "🤖 Detected intent: get_agenda\n",
            "📝 What is the agenda of the workshop/meeting?: swimming\n",
            "🧠 Generating banner image...\n",
            "🖼️ Image created: https://oaidalleapiprodscus.blob.core.windows.net/private/org-39LAhQ4efzWBgsvKRvFs3Rao/user-PwfybX5L6lUZPyfB2uzqiWs8/img-93A03hmESnrdrQzY9RhH5p86.png?st=2025-07-11T14%3A55%3A33Z&se=2025-07-11T16%3A55%3A33Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=4ab8dc02-4155-4914-980a-5346f458538c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-07-10T20%3A35%3A32Z&ske=2025-07-11T20%3A35%3A32Z&sks=b&skv=2024-08-04&sig=a4VRES%2BRNZIWZRoHp%2BngE0PXsVipvkqst%2Ba0lI1x6ek%3D\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-39LAhQ4efzWBgsvKRvFs3Rao/user-PwfybX5L6lUZPyfB2uzqiWs8/img-93A03hmESnrdrQzY9RhH5p86.png?st=2025-07-11T14%3A55%3A33Z&se=2025-07-11T16%3A55%3A33Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=4ab8dc02-4155-4914-980a-5346f458538c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-07-10T20%3A35%3A32Z&ske=2025-07-11T20%3A35%3A32Z&sks=b&skv=2024-08-04&sig=a4VRES%2BRNZIWZRoHp%2BngE0PXsVipvkqst%2Ba0lI1x6ek%3D\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💬 What would you like to do next? new city\n",
            "🤖 Detected intent: ask_city\n",
            "\n",
            "📍 Cities found:\n",
            "1. London\n",
            "2. New York\n",
            "3. Houston\n",
            "4. Singapore\n",
            "5. Hong Kong\n",
            "6. Dhaka\n",
            "7. Paris\n",
            "8. Frankfurt\n",
            "9. Warsaw\n",
            "10. Stockholm\n",
            "11. Nairobi\n",
            "12. Accra\n",
            "13. Gaborone\n",
            "14. Port Louis\n",
            "15. Lagos\n",
            "16. Lusaka\n",
            "17. Dar-es-Salaam\n",
            "18. Kampala\n",
            "19. Ho Chi Minh City\n",
            "20. Yangon\n",
            "21. Colombo\n",
            "22. Kathmandu\n",
            "23. Karachi\n",
            "24. Doha\n",
            "25. Riyadh\n",
            "26. Dubai\n",
            "27. Baghdad\n",
            "28. Bahrain\n",
            "29. Abidjan\n",
            "30. Cairo\n",
            "Choose a city NUMBER: 24\n",
            "🌈 Weather Info: 🌤️ Doha: 33.2°C, Sunny\n",
            "\n",
            "🔎 Generating event recommendation...\n",
            "✅ Recommendation: Indoor swimming pool would be a suitable setting for the workshop in Doha due to the hot weather.\n",
            "📝 What is the agenda of the workshop/meeting?: beer party\n",
            "🧠 Generating banner image...\n",
            "🖼️ Image created: https://oaidalleapiprodscus.blob.core.windows.net/private/org-39LAhQ4efzWBgsvKRvFs3Rao/user-PwfybX5L6lUZPyfB2uzqiWs8/img-4uSXbhVkfwiBUUyGQxTOtUQM.png?st=2025-07-11T14%3A56%3A29Z&se=2025-07-11T16%3A56%3A29Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=4ab8dc02-4155-4914-980a-5346f458538c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-07-11T03%3A53%3A24Z&ske=2025-07-12T03%3A53%3A24Z&sks=b&skv=2024-08-04&sig=I2EI6Q9sjMjAvgl2Y7bK28UnB1BcupJ2I5/3uBaMF/M%3D\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-39LAhQ4efzWBgsvKRvFs3Rao/user-PwfybX5L6lUZPyfB2uzqiWs8/img-4uSXbhVkfwiBUUyGQxTOtUQM.png?st=2025-07-11T14%3A56%3A29Z&se=2025-07-11T16%3A56%3A29Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=4ab8dc02-4155-4914-980a-5346f458538c&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-07-11T03%3A53%3A24Z&ske=2025-07-12T03%3A53%3A24Z&sks=b&skv=2024-08-04&sig=I2EI6Q9sjMjAvgl2Y7bK28UnB1BcupJ2I5/3uBaMF/M%3D\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💬 What would you like to do next? options\n",
            "⚠️ Sorry, I didn't understand that. You can try:\n",
            "- 'Change agenda'\n",
            "- 'New city'\n",
            "- 'Ask about the PDF'\n",
            "- 'Exit'\n",
            "🤖 Detected intent: classify_intent\n",
            "💬 What would you like to do next? Exit\n",
            "🤖 Detected intent: end\n"
          ]
        }
      ]
    }
  ]
}